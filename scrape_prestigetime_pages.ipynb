{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Watch Pages\n",
    "\n",
    "* Script goes through the individual watch pages\n",
    "* Saves the html locally (for future)\n",
    "* Extracts Attributes from Watches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# General Libraries\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import csv\n",
    "\n",
    "# Analysis Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Scraping Libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from fake_useragent import UserAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomize Me to Prevent Getting Blocked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_soup(url):\n",
    "    us = UserAgent()\n",
    "    user_agent = {'User-Agent':us.random}\n",
    "    response = requests.get(url,headers = user_agent)\n",
    "    page = response.text\n",
    "    soup = BeautifulSoup(page,\"lxml\")\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_stats_dict(watch_soup):\n",
    "    stats = watch_soup.find('table',class_=\"table table-condensed item-table\")\n",
    "\n",
    "    # Table of Attributes\n",
    "\n",
    "    stats_dict = dict()\n",
    "    stats_dict['url'] = url\n",
    "    \n",
    "    for jx, row in enumerate(stats.find_all('tr')):\n",
    "        tds = row.find_all('td')\n",
    "        k = tds[0].text\n",
    "        v = tds[1].text\n",
    "\n",
    "        bad_vals = ['30m/99ft - 50m/165ft',\n",
    "                    '50m/165ft - 100m/330ft',\n",
    "                    '100m/330ft - 200m/660ft',\n",
    "                    '200m/660ft - 500m/1650ft',\n",
    "                    '500m/1650ft +',\n",
    "                   'No rating']\n",
    "        if k == 'Movement':\n",
    "            stats_dict[k] = v.split('\\n')[0]\n",
    "        elif k in bad_vals:\n",
    "            pass\n",
    "        elif 'Water' in k:\n",
    "            stats_dict['Water Resistance'] = row.find_all('td')[-1].text\n",
    "        else:\n",
    "            stats_dict[k] = v\n",
    "        \n",
    "    return stats_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__PrestigeTime__ \n",
    "\n",
    "Get Individual Watch Listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gender = 'mens'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "watch_DF = pd.read_csv('watch_page_list_' + gender + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "urls = watch_DF['url']\n",
    "save_path = 'prestige_time_pages_' + gender + '/'\n",
    "aggregated_save_name = 'additional_stats_' + gender + '.csv' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "additional_stats_list = []\n",
    "for ix, url in enumerate(urls):\n",
    "    time.sleep(1+np.random.uniform(0,2)) \n",
    "    \n",
    "    watch_soup = random_soup(url)    \n",
    "    stats_dict = make_stats_dict(watch_soup)        \n",
    "    additional_stats_list.append(stats_dict)\n",
    "    \n",
    "    # Save file to computer\n",
    "    # to prevent future headache\n",
    "    save_html = watch_soup.prettify(\"utf-8\")\n",
    "    \n",
    "    save_name = watch_DF.loc[\n",
    "        watch_DF['url'] == url,:]['image_name'].values[0] + '.html'\n",
    "   \n",
    "\n",
    "    with open(save_path + save_name, \"wb\") as file:\n",
    "        file.write(save_html)\n",
    "        file.close()\n",
    "    \n",
    "    if ix % 50 == 0:\n",
    "        print(ix)\n",
    "    \n",
    "\n",
    "additional_stats_df = pd.DataFrame(additional_stats_list)\n",
    "additional_stats_df.to_csv(aggregated_save_name,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "additional_stats = pd.read_csv(aggregated_save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# html_f = open(save_path + save_name,'r')\n",
    "# page = html_f.read()\n",
    "# testsoup = BeautifulSoup(page,\"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
